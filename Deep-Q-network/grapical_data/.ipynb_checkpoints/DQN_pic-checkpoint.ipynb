{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference:https://zhuanlan.zhihu.com/p/21477488?refer=intelligentunit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.finance as finance\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from yahoo_finance import Share\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## day_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_len = 10    # numbers of days for every data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of data: 1289\n",
      "after removing the datas with zero volume, the length of data: 1223\n",
      "the length of data: 161\n",
      "after removing the datas with zero volume, the length of data: 147\n"
     ]
    }
   ],
   "source": [
    "def get_stock(ticker, startdate, enddate):\n",
    "    fh = finance.fetch_historical_yahoo(ticker, startdate, enddate)\n",
    "    # a numpy record array with fields: date, open, high, low, close, volume, adj_close)\n",
    "    r = mlab.csv2rec(fh)\n",
    "    fh.close()\n",
    "    r.sort()\n",
    "    print 'the length of data:', len(r.close)\n",
    "    get_stock_data = []\n",
    "    for i in xrange(0, len(r.close)-1):\n",
    "        if (r.volume[i] != 0):\n",
    "            get_stock_data.append(r.close[i].tolist())\n",
    "    print 'after removing the datas with zero volume, the length of data:', len(get_stock_data)\n",
    "    return get_stock_data\n",
    "\n",
    "ticker = '2330.TW'\n",
    "\n",
    "train = get_stock(ticker, datetime.date(2011, 1, 1), datetime.date(2015, 12, 31))\n",
    "test = get_stock(ticker, datetime.date(2016, 1, 1), datetime.date(2016, 8, 17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cal relative price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07391304347826087, -0.06919431279620851, 1222, 146, 0.013368983957219251)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_relative_data(stock_data):\n",
    "    relative_data = []\n",
    "    for i in xrange(1, len(stock_data)):\n",
    "        relative_price_change = (stock_data[i] - stock_data[i-1]) / stock_data[i-1]\n",
    "        relative_data.append(relative_price_change)\n",
    "    return relative_data\n",
    "relative_train = get_relative_data(train)\n",
    "relative_test = get_relative_data(test)\n",
    "\n",
    "max_ylim = max(max(relative_train), max(relative_test))\n",
    "min_ylim = min(min(relative_train), min(relative_test))\n",
    "max_ylim, min_ylim, len(relative_train), len(relative_test), relative_train[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and save relative price line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef save_pic(data, filename):\\n    for i in xrange (0, len(data)-day_len):\\n        fig, ax = plt.subplots(nrows=1, ncols=1)\\n        fig.set_size_inches(1, 1)\\n        ax.plot([i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9], [data[i], data[i+1], data[i+2], data[i+3], data[i+4], data[i+5], data[i+6], data[i+7], data[i+8], data[i+9]])\\n        ax.set_ylim([min_ylim, max_ylim])\\n        plt.axis(\\'off\\')\\n        fig.savefig(\\'/home/carine/Desktop/tmp/\\'+filename+\\'/\\'+filename+\\'_\\'+str(i)+\\'.png\\', dpi=80)\\n        fig.clear()\\n        plt.close(fig)\\n\\nsave_pic(relative_train, \"train\")\\nsave_pic(relative_test, \"test\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def save_pic(data, filename):\n",
    "    for i in xrange (0, len(data)-day_len):\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "        fig.set_size_inches(1, 1)\n",
    "        ax.plot([i, i+1, i+2, i+3, i+4, i+5, i+6, i+7, i+8, i+9], [data[i], data[i+1], data[i+2], data[i+3], data[i+4], data[i+5], data[i+6], data[i+7], data[i+8], data[i+9]])\n",
    "        ax.set_ylim([min_ylim, max_ylim])\n",
    "        plt.axis('off')\n",
    "        fig.savefig('/home/carine/Desktop/tmp/'+filename+'/'+filename+'_'+str(i)+'.png', dpi=80)\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "\n",
    "save_pic(relative_train, \"train\")\n",
    "save_pic(relative_test, \"test\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and recolor relative line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_image(file_dir):\n",
    "    img = mpimg.imread(file_dir)\n",
    "    return img\n",
    "\n",
    "image = []\n",
    "for i in xrange(0, len(relative_train)-day_len):\n",
    "    file_dir = \"/home/carine/Desktop/2330_2011-2016/relative_price/train/train_\" + str(i) + \".png\"\n",
    "    image.append(get_image(file_dir))\n",
    "my_train = np.asarray(image)\n",
    "\n",
    "# recolor from RGBA to (0, 1)\n",
    "my_train_image = np.ndarray(shape=(len(my_train), 80, 80), dtype = np.int_)\n",
    "for i in xrange (0, len(my_train)):\n",
    "    for j in xrange(0, 80):\n",
    "        for k in xrange(0, 80):\n",
    "            if my_train[i][j][k][0] != 1 or my_train[i][j][k][1] != 1 or my_train[i][j][k][2] != 1 or my_train[i][j][k][3] != 1:\n",
    "                my_train[i][j][k][0] = 0\n",
    "            my_train_image[i][j][k] = my_train[i][j][k][0]\n",
    "            \n",
    "\n",
    "image = []\n",
    "for i in xrange(0, len(relative_test)-day_len):\n",
    "    file_dir = \"/home/carine/Desktop/2330_2011-2016/relative_price/test/test_\" + str(i) + \".png\"\n",
    "    image.append(get_image(file_dir))\n",
    "my_test = np.asarray(image)\n",
    "\n",
    "# recolor from RGBA to (0, 1)\n",
    "my_test_image = np.ndarray(shape=(len(my_test), 80, 80), dtype = np.int_)\n",
    "for i in xrange (0, len(my_test)):\n",
    "    for j in xrange(0, 80):\n",
    "        for k in xrange(0, 80):\n",
    "            if my_test[i][j][k][0] != 1 or my_test[i][j][k][1] != 1 or my_test[i][j][k][2] != 1 or my_test[i][j][k][3] != 1:\n",
    "                my_test[i][j][k][0] = 0\n",
    "            my_test_image[i][j][k] = my_test[i][j][k][0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_stock_train = np.zeros((len(relative_train)-day_len, day_len), dtype=np.float)\n",
    "my_stock_test = np.zeros((len(relative_test)-day_len, day_len), dtype=np.float)\n",
    "for i in xrange(0, len(my_stock_train)):\n",
    "    for j in xrange(0, day_len):\n",
    "        my_stock_train[i,j] = train[i+j+1]\n",
    "\n",
    "for i in xrange(0, len(my_stock_test)):\n",
    "    for j in xrange(0, day_len):\n",
    "        my_stock_test[i,j] = test[i+j+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tw stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TWStock():\n",
    "    def __init__(self, image_data, stock_price):\n",
    "        self.image_data = image_data\n",
    "        self.stock_price = stock_price\n",
    "        self.stock_index = 0\n",
    "    \n",
    "    def render(self):\n",
    "        return \n",
    "    \n",
    "    def reset(self):\n",
    "        self.stock_index = 0\n",
    "        return self.image_data[self.stock_index]\n",
    "    \n",
    "    # 0:do nothing, 1:buy and then sell, 2:sell and then buy\n",
    "    def step(self, action): \n",
    "        self.stock_index += 1\n",
    "        action_reward = self.stock_price[self.stock_index][day_len-1] - self.stock_price[self.stock_index][day_len-2] \n",
    "        if (action == 0):\n",
    "            action_reward = 0\n",
    "        #if (action == 2):\n",
    "        #    action_reward = -1 * action_reward\n",
    "\n",
    "        stock_done = False\n",
    "        if self.stock_index >= len(self.image_data)-1:\n",
    "            stock_done = True\n",
    "        else:\n",
    "            stock_done = False\n",
    "        return self.image_data[self.stock_index], action_reward, stock_done, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    # DQN Agent\n",
    "    def __init__(self, env):\n",
    "        # init experience replay\n",
    "        self.replay_buffer = deque()\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "    \n",
    "        #self.state_dim = env.observation_space.shape[0]\n",
    "        #self.action_dim = env.action_space.n\n",
    "    \n",
    "        self.state_dim = [None, 80, 80]\n",
    "        self.action_dim = 2\n",
    "\n",
    "\n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "\n",
    "\n",
    "        #g_record = tf.Graph()\n",
    "        #self.g_session = tf.InteractiveSession(graph=g_record)\n",
    "        self.t_session = tf.InteractiveSession()\n",
    "\n",
    "        #with g_record.as_default():\n",
    "        #self.R = tf.placeholder(\"float\", shape = None)\n",
    "        #R_summ = tf.scalar_summary(tags =\"reward\", values = self.R)\n",
    "\n",
    "        #self.merged_summ = tf.merge_all_summaries()\n",
    "        \n",
    "        #self.writer = tf.train.SummaryWriter('/home/carine/Desktop/stock_DQN/eventlog',graph=self.t_session.graph)\n",
    "\n",
    "        self.t_session.run(tf.initialize_all_variables())\n",
    "\n",
    "    #def get_summ(self):\n",
    "        #return self.t_session, self.merged_summ, self.R, self.writer\n",
    "\n",
    "    def create_Q_network(self):\n",
    "        # input layer\n",
    "        self.state_input = tf.placeholder(tf.float32,[None ,80, 80])\n",
    "        x_image = tf.reshape(self.state_input, [-1, 80, 80, 1]) \n",
    "        \n",
    "        # network weights\n",
    "        W_conv1 = self.weight_variable([8, 8, 1, 32])\n",
    "        b_conv1 = self.bias_variable([32])\n",
    "        \n",
    "        W_conv2 = self.weight_variable([4, 4, 32, 64])\n",
    "        b_conv2 = self.bias_variable([64])\n",
    "\n",
    "        W_conv3 = self.weight_variable([3, 3, 64, 64])\n",
    "        b_conv3 = self.bias_variable([64])\n",
    "        \n",
    "        \n",
    "        W_fc1 = self.weight_variable([1600, 512])\n",
    "        b_fc1 = self.bias_variable([512])\n",
    "        \n",
    "        W_fc2 = self.weight_variable([512, self.action_dim])\n",
    "        b_fc2 = self.bias_variable([self.action_dim])\n",
    "        \n",
    "        # hidden layers\n",
    "        h_conv1 = tf.nn.relu(self.conv2d(x_image, W_conv1, 4) + b_conv1)\n",
    "        h_pool1 = self.max_pool_2x2(h_conv1)\n",
    "        h_conv2 = tf.nn.relu(self.conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "        h_conv3 = tf.nn.relu(self.conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "        # Q Value layer\n",
    "        self.Q_value = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "        print self.Q_value.get_shape()\n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(tf.float32,[None,self.action_dim])\n",
    "        # one hot presentation\n",
    "        self.y_input = tf.placeholder(tf.float32,[None])\n",
    "        Q_action = tf.reduce_sum(tf.mul(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n",
    "\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "        self.optimizer.run(feed_dict={\n",
    "          self.y_input: y_batch,\n",
    "          self.action_input: action_batch,\n",
    "          self.state_input: state_batch\n",
    "          })\n",
    "        \n",
    "\n",
    "    def egreedy_action(self,state):\n",
    "        Q_value = self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]})[0]\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0,self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "\n",
    "        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n",
    "\n",
    "\n",
    "    def action(self,state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]})[0])\n",
    "\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv2d(self, x, W, stride):\n",
    "        return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "    def max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "ENV_NAME = 'stock_2330-v0' \n",
    "EPISODE = 10000 #10000 # Episode limitation\n",
    "STEP = len(train) #1000   #300 # Step limitation in an episode\n",
    "TEST = 10 #10 # The number of experiment test every 100 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    #env = gym.make(ENV_NAME)\n",
    "    env = TWStock(my_train_image,my_stock_train) \n",
    "    agent = DQN(env)\n",
    "    #sess,merged,R,writer = agent.get_summ()\n",
    "    \n",
    "    \n",
    "    print 'Start!'\n",
    "    for episode in xrange(EPISODE):\n",
    "    \n",
    "        # initialize task\n",
    "        state = env.reset()\n",
    "        train_reward = 0\n",
    "        # Train\n",
    "        for step in xrange(STEP):\n",
    "            \n",
    "            action = agent.egreedy_action(state) # e-greedy action for trai\n",
    "\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            train_reward += reward\n",
    "            # Define reward for agent\n",
    "            reward_agent = -1 if done else 0.1\n",
    "            agent.perceive(state,action,reward,next_state,done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break  \n",
    "        if episode % 10 == 0:\n",
    "            print 'training episode:', episode, 'Evalutaion Average Reward:', train_reward\n",
    "\n",
    "        # Test every 100 episodes\n",
    "        if episode % 10 == 0:\n",
    "            env_test = TWStock(my_test_image, my_stock_test)\n",
    "            total_reward = 0\n",
    "\n",
    "            for i in xrange(TEST):\n",
    "                state = env_test.reset()\n",
    "\n",
    "                for j in xrange(STEP):\n",
    "                    env_test.render()\n",
    "                    action = agent.action(state)   # direct action for test\n",
    "                    state,reward,done,_ = env_test.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "            ave_reward = total_reward/TEST\n",
    "            print 'testing episode: ',episode,'Evaluation Average Reward:',ave_reward\n",
    "            #record = sess.run(merged, feed_dict={R:ave_reward})\n",
    "            #writer.add_summary(record, global_step = episode)\n",
    "            #writer.flush()\n",
    "            #if ave_reward >= 200:\n",
    "            #    print 'Done!' \n",
    "            #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n",
      "Start!\n",
      "training episode: 0 Evalutaion Average Reward: -28.8\n",
      "testing episode:  0 Evaluation Average Reward: 0\n",
      "training episode: 10 Evalutaion Average Reward: 249.2\n",
      "testing episode:  10 Evaluation Average Reward: 34.5\n",
      "training episode: 20 Evalutaion Average Reward: 389.5\n",
      "testing episode:  20 Evaluation Average Reward: 18.5\n",
      "training episode: 30 Evalutaion Average Reward: 389.5\n",
      "testing episode:  30 Evaluation Average Reward: 19.5\n",
      "training episode: 40 Evalutaion Average Reward: 389.3\n",
      "testing episode:  40 Evaluation Average Reward: 12.5\n",
      "training episode: 50 Evalutaion Average Reward: 428.2\n",
      "testing episode:  50 Evaluation Average Reward: 18.0\n",
      "training episode: 60 Evalutaion Average Reward: 370.4\n",
      "testing episode:  60 Evaluation Average Reward: 11.0\n",
      "training episode: 70 Evalutaion Average Reward: 413.3\n",
      "testing episode:  70 Evaluation Average Reward: 16.5\n",
      "training episode: 80 Evalutaion Average Reward: 427.2\n",
      "testing episode:  80 Evaluation Average Reward: 12.5\n",
      "training episode: 90 Evalutaion Average Reward: 408.4\n",
      "testing episode:  90 Evaluation Average Reward: 11.5\n",
      "training episode: 100 Evalutaion Average Reward: 428.4\n",
      "testing episode:  100 Evaluation Average Reward: 22.5\n",
      "training episode: 110 Evalutaion Average Reward: 401.4\n",
      "testing episode:  110 Evaluation Average Reward: 17.5\n",
      "training episode: 120 Evalutaion Average Reward: 383.5\n",
      "testing episode:  120 Evaluation Average Reward: 18.0\n",
      "training episode: 130 Evalutaion Average Reward: 373.0\n",
      "testing episode:  130 Evaluation Average Reward: 21.5\n",
      "training episode: 140 Evalutaion Average Reward: 396.9\n",
      "testing episode:  140 Evaluation Average Reward: 19.5\n",
      "training episode: 150 Evalutaion Average Reward: 344.4\n",
      "testing episode:  150 Evaluation Average Reward: 18.0\n",
      "training episode: 160 Evalutaion Average Reward: 416.4\n",
      "testing episode:  160 Evaluation Average Reward: 21.0\n",
      "training episode: 170 Evalutaion Average Reward: 354.7\n",
      "testing episode:  170 Evaluation Average Reward: 17.5\n",
      "training episode: 180 Evalutaion Average Reward: 445.7\n",
      "testing episode:  180 Evaluation Average Reward: 21.0\n",
      "training episode: 190 Evalutaion Average Reward: 379.8\n",
      "testing episode:  190 Evaluation Average Reward: 22.5\n",
      "training episode: 200 Evalutaion Average Reward: 420.0\n",
      "testing episode:  200 Evaluation Average Reward: 19.0\n",
      "training episode: 210 Evalutaion Average Reward: 375.6\n",
      "testing episode:  210 Evaluation Average Reward: 22.5\n",
      "training episode: 220 Evalutaion Average Reward: 418.9\n",
      "testing episode:  220 Evaluation Average Reward: 23.5\n",
      "training episode: 230 Evalutaion Average Reward: 390.5\n",
      "testing episode:  230 Evaluation Average Reward: 23.5\n",
      "training episode: 240 Evalutaion Average Reward: 364.7\n",
      "testing episode:  240 Evaluation Average Reward: 23.5\n",
      "training episode: 250 Evalutaion Average Reward: 374.8\n",
      "testing episode:  250 Evaluation Average Reward: 22.0\n",
      "training episode: 260 Evalutaion Average Reward: 401.8\n",
      "testing episode:  260 Evaluation Average Reward: 23.5\n",
      "training episode: 270 Evalutaion Average Reward: 383.6\n",
      "testing episode:  270 Evaluation Average Reward: 22.5\n",
      "training episode: 280 Evalutaion Average Reward: 397.3\n",
      "testing episode:  280 Evaluation Average Reward: 23.0\n",
      "training episode: 290 Evalutaion Average Reward: 388.3\n",
      "testing episode:  290 Evaluation Average Reward: 18.5\n",
      "training episode: 300 Evalutaion Average Reward: 407.2\n",
      "testing episode:  300 Evaluation Average Reward: 22.0\n",
      "training episode: 310 Evalutaion Average Reward: 400.4\n",
      "testing episode:  310 Evaluation Average Reward: 20.0\n",
      "training episode: 320 Evalutaion Average Reward: 392.8\n",
      "testing episode:  320 Evaluation Average Reward: 19.5\n",
      "training episode: 330 Evalutaion Average Reward: 396.3\n",
      "testing episode:  330 Evaluation Average Reward: 21.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2436fc2ab63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-e12485d69457>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Define reward for agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mreward_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f6d2b7d6b16b>\u001b[0m in \u001b[0;36mperceive\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Q_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_Q_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f6d2b7d6b16b>\u001b[0m in \u001b[0;36mtrain_Q_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m           })\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3531\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3532\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3533\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
